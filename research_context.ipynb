{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you encounter errors in the code, update the packages\n",
    "# !pip install scipy==1.10.1\n",
    "# !pip install numpy==1.21.2\n",
    "# !pip install pandas==1.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saver_func import SaveData\n",
    "\n",
    "save = SaveData.save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You will have **30** total minutes to look through four tasks (10 for tasks 0 and 1, 10 for task 2, and 10 for task 3). Your goal is to understand the purpose of the code, as if to be asked similar tasks. You should strive not only to understand what the code is doing, but also why the code's author made the choices they did. We will follow up this process of discovery with questions asking about your understanding of the material.\n",
    "\n",
    "Please, do **not** write any additional Pandas code. In order to view intermediate DataFrames or Series, you may make a new line with the function `save()` to save any DataFrame or Series. An example using `Task 0` is seen here:\n",
    "\n",
    "> ```python\n",
    "> buoy_raw = pd.read_csv(fp, sep='\\s+')\n",
    "> save(buoy_raw)\n",
    "> buoy = buoy_raw.dropna()\n",
    "> ```\n",
    "\n",
    "You are welcome to run the cells as many times as you need. Further, you may use the `save()` function as many times as you see fit. Be careful not to adjust any code. If you edit any lines, the original code is hidden in a markdown cell at the top of the notebook.\n",
    "\n",
    "We will interrupt you after 10 minutes to prompt you to complete a [short survey](https://forms.gle/7jdxEDTrbd1EVRHT9) and move on to the next section. If you finish a section early, let us know before you move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "You have joined a lab at the Scripps Institution of Oceanography. Your coworker, Alex, was tasked to make a quick analysis on a dataset and has since left for vacation. Your boss has asked you to get familiar with Alex's code in order to make additional analysis. Unfortunately for you, their code lacks comments or much neatness. Become familiar with Alex's code for future analysis. Subject matter is not necessary for the analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Broadly, the four tasks are as follows\n",
    "- **Task 0**: Clean the dataset\n",
    "- **Task 1**: Assess missingness\n",
    "    - Definition: Data is considered \"missing at random\" if the chance that a value is missing depends on other columns, but not the actual missing value itself.\n",
    "- **Task 2**: Imputation\n",
    "- **Task 3**: Compare imputation to original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 0\n",
    "\n",
    "# Link to txt file: https://raw.githubusercontent.com/ch-lum/table_information/main/buoy.txt\n",
    "# Link to schema: https://www.ndbc.noaa.gov/obsdes.shtml\n",
    "\n",
    "fp = 'buoy.txt'\n",
    "buoy_raw = pd.read_csv(fp, sep='\\s+')\n",
    "buoy = buoy_raw.dropna()\n",
    "buoy = buoy.replace(r\"-+$\", np.nan, regex=True)\n",
    "buoy = buoy[buoy[\"TIME\"] != \"TIME\"]\n",
    "buoy = buoy.dropna(axis=1, how='all').reset_index(drop=True)\n",
    "non_numeric_cols = set(['D', 'T1', 'SwD', 'WWD', 'STEEPNESS'])\n",
    "numeric_cols = list(set(buoy.columns) - non_numeric_cols)\n",
    "buoy.loc[:, numeric_cols] = buoy.loc[:, numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "buoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 1\n",
    "\n",
    "totals = buoy[\"T1\"].value_counts()\n",
    "missing_some = buoy.drop(buoy.columns[~buoy.isnull().any()].drop(\"T1\"), axis=1)\n",
    "present_counts = missing_some.groupby(\"T1\").count()\n",
    "cont_tables = present_counts.melt(ignore_index=False).rename(\n",
    "    columns={\"value\": \"Present\"}\n",
    ")\n",
    "cont_tables[\"Missing\"] = cont_tables.groupby(\"variable\")[\"Present\"].transform(\n",
    "    lambda x: totals - x\n",
    ")\n",
    "missingness = cont_tables.groupby(\"variable\").apply(\n",
    "    lambda x: stats.chi2_contingency(x).pvalue < 0.0001\n",
    ")\n",
    "present_props = present_counts.div(totals, axis=0)\n",
    "present_props.T.plot(kind=\"bar\").axhline(\n",
    "    y=1.0, color=\"red\", linestyle=\"--\", linewidth=0.5\n",
    ")\n",
    "\n",
    "missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 2\n",
    "\n",
    "no_ships = buoy.groupby(\"T1\").filter(lambda x: len(x) > 200)\n",
    "melted = no_ships.melt(id_vars=\"T1\", ignore_index=False)\n",
    "melted = melted.groupby(\"variable\").filter(\n",
    "    lambda x: x.groupby(\"T1\")[\"value\"].count().min() > 200\n",
    ")\n",
    "un_melted = melted.reset_index().pivot(\n",
    "    index=\"index\", columns=\"variable\", values=\"value\"\n",
    ")\n",
    "no_ships = pd.concat([no_ships[\"T1\"], un_melted], axis=1).reset_index(drop=True)\n",
    "quantiles = pd.qcut(no_ships[\"TIME\"], 25)\n",
    "missing_cols = no_ships.isnull().any()\n",
    "means = no_ships.groupby([quantiles, \"T1\"])[\n",
    "    no_ships.columns[missing_cols]\n",
    "].mean()\n",
    "imputation = (\n",
    "    pd.merge(\n",
    "        no_ships.set_index([quantiles, \"T1\"]).loc[:, \"D\"],\n",
    "        means,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    .loc[:, missing_cols]\n",
    "    .reset_index()\n",
    ")\n",
    "imputed = no_ships.copy()\n",
    "imputed.loc[:, missing_cols] = no_ships.loc[:, missing_cols].combine_first(\n",
    "    imputation.loc[:, missing_cols]\n",
    ")\n",
    "\n",
    "imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 3\n",
    "\n",
    "together = pd.concat([imputed, no_ships], keys=[\"imputed\", \"original\"])\n",
    "together = together[(together[\"TIME\"] >= 900) & (together[\"TIME\"] <= 2100)]\n",
    "together = together[[\"TIME\", \"WDIR\"]]\n",
    "together.loc[:, \"WDIR_adj\"] = (together[\"WDIR\"] + (360 / 16 / 2)) % 360\n",
    "together = together.reset_index()\n",
    "directions = pd.cut(\n",
    "    together[\"WDIR_adj\"], bins=np.linspace(0, 360, 17), include_lowest=True\n",
    ")\n",
    "hours = together[\"TIME\"] // 100\n",
    "d_h = together.pivot_table(\n",
    "    index=[\"level_0\", hours],\n",
    "    columns=directions,\n",
    "    values=\"TIME\",\n",
    "    aggfunc=\"count\",\n",
    ")\n",
    "d_h.columns = [\n",
    "    \"N\",\n",
    "    \"NNE\",\n",
    "    \"NE\",\n",
    "    \"ENE\",\n",
    "    \"E\",\n",
    "    \"ESE\",\n",
    "    \"SE\",\n",
    "    \"SSE\",\n",
    "    \"S\",\n",
    "    \"SSW\",\n",
    "    \"SW\",\n",
    "    \"WSW\",\n",
    "    \"W\",\n",
    "    \"WNW\",\n",
    "    \"NW\",\n",
    "    \"NNW\",\n",
    "]\n",
    "d_h = d_h.fillna(0)\n",
    "d_h_props = d_h.div(d_h.sum(axis=1), axis=0)\n",
    "\n",
    "for key in d_h.index.levels[0]:\n",
    "    axes = d_h_props.xs(key).plot(kind=\"area\", colormap=\"twilight_shifted\")\n",
    "    axes.set_title(key)\n",
    "    axes.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code (in markdown)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "################################################################\n",
    "\n",
    "### TASK 0\n",
    "\n",
    "# Link to txt file: https://raw.githubusercontent.com/ch-lum/table_information/main/buoy.txt\n",
    "# Link to schema: https://www.ndbc.noaa.gov/obsdes.shtml\n",
    "\n",
    "fp = 'buoy.txt'\n",
    "buoy_raw = pd.read_csv(fp, sep='\\s+')\n",
    "buoy = buoy_raw.dropna()\n",
    "buoy = buoy.replace(r\"-+$\", np.nan, regex=True)\n",
    "buoy = buoy[buoy[\"TIME\"] != \"TIME\"]\n",
    "buoy = buoy.dropna(axis=1, how='all').reset_index(drop=True)\n",
    "buoy.iloc[:, 2:-5] = buoy.iloc[:, 2:-5].apply(pd.to_numeric, errors='coerce')\n",
    "buoy.iloc[:, -4:-2] = buoy.iloc[:, -4:-2].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "buoy\n",
    "\n",
    "################################################################\n",
    "\n",
    "### TASK 1\n",
    "\n",
    "totals = buoy[\"T1\"].value_counts()\n",
    "missing_some = buoy.drop(buoy.columns[~buoy.isnull().any()].drop(\"T1\"), axis=1)\n",
    "present_counts = missing_some.groupby(\"T1\").count()\n",
    "cont_tables = present_counts.melt(ignore_index=False).rename(\n",
    "    columns={\"value\": \"Present\"}\n",
    ")\n",
    "cont_tables[\"Missing\"] = cont_tables.groupby(\"variable\")[\"Present\"].transform(\n",
    "    lambda x: totals - x\n",
    ")\n",
    "missingness = cont_tables.groupby(\"variable\").apply(\n",
    "    lambda x: stats.chi2_contingency(x).pvalue< 0.0001\n",
    ")\n",
    "present_props = present_counts.div(totals, axis=0)\n",
    "present_props.T.plot(kind=\"bar\").axhline(\n",
    "    y=1.0, color=\"red\", linestyle=\"--\", linewidth=0.5\n",
    ")\n",
    "\n",
    "missingness\n",
    "\n",
    "################################################################\n",
    "\n",
    "### TASK 2\n",
    "\n",
    "no_ships = buoy.groupby(\"T1\").filter(lambda x: len(x) > 200)\n",
    "melted = no_ships.melt(id_vars=\"T1\", ignore_index=False)\n",
    "melted = melted.groupby(\"variable\").filter(\n",
    "    lambda x: x.groupby(\"T1\")[\"value\"].count().min() > 200\n",
    ")\n",
    "un_melted = melted.reset_index().pivot(\n",
    "    index=\"index\", columns=\"variable\", values=\"value\"\n",
    ")\n",
    "no_ships = pd.concat([no_ships[\"T1\"], un_melted], axis=1).reset_index(drop=True)\n",
    "quantiles = pd.qcut(no_ships[\"TIME\"], 25)\n",
    "missing_cols = no_ships.isnull().any()\n",
    "means = no_ships.groupby([quantiles, \"T1\"])[\n",
    "    no_ships.columns[missing_cols]\n",
    "].mean()\n",
    "imputation = (\n",
    "    pd.merge(\n",
    "        no_ships.set_index([quantiles, \"T1\"]).loc[:, \"D\"],\n",
    "        means,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    .loc[:, missing_cols]\n",
    "    .reset_index()\n",
    ")\n",
    "imputed = no_ships.copy()\n",
    "imputed.loc[:, missing_cols] = no_ships.loc[:, missing_cols].combine_first(\n",
    "    imputation.loc[:, missing_cols]\n",
    ")\n",
    "\n",
    "imputed\n",
    "\n",
    "################################################################\n",
    "\n",
    "### TASK 3\n",
    "\n",
    "together = pd.concat([imputed, no_ships], keys=[\"imputed\", \"original\"])\n",
    "together = together[(together[\"TIME\"] >= 900) & (together[\"TIME\"] <= 2100)]\n",
    "together = together[[\"TIME\", \"WDIR\"]]\n",
    "together.loc[:, \"WDIR_adj\"] = (together[\"WDIR\"] + (360 / 16 / 2)) % 360\n",
    "together = together.reset_index()\n",
    "directions = pd.cut(\n",
    "    together[\"WDIR_adj\"], bins=np.linspace(0, 360, 17), include_lowest=True\n",
    ")\n",
    "hours = together[\"TIME\"] // 100\n",
    "d_h = together.pivot_table(\n",
    "    index=[\"level_0\", hours],\n",
    "    columns=directions,\n",
    "    values=\"TIME\",\n",
    "    aggfunc=\"count\",\n",
    ")\n",
    "d_h.columns = [\n",
    "    \"N\",\n",
    "    \"NNE\",\n",
    "    \"NE\",\n",
    "    \"ENE\",\n",
    "    \"E\",\n",
    "    \"ESE\",\n",
    "    \"SE\",\n",
    "    \"SSE\",\n",
    "    \"S\",\n",
    "    \"SSW\",\n",
    "    \"SW\",\n",
    "    \"WSW\",\n",
    "    \"W\",\n",
    "    \"WNW\",\n",
    "    \"NW\",\n",
    "    \"NNW\",\n",
    "]\n",
    "d_h = d_h.fillna(0)\n",
    "d_h_props = d_h.div(d_h.sum(axis=1), axis=0)\n",
    "\n",
    "for key in d_h.index.levels[0]:\n",
    "    axes = d_h_props.xs(key).plot(kind=\"area\", colormap=\"twilight_shifted\")\n",
    "    axes.set_title(key)\n",
    "    axes.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
